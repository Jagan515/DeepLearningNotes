# üìò RECURRENT NEURAL NETWORK (RNN) ‚Äî COMPLETE NOTES

*(ONLY Simple / Vanilla RNN)*

![Image](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)

![Image](https://www.researchgate.net/publication/346853926/figure/fig2/AS%3A1007113883119616%401617126069530/A-simple-RNN-with-a-single-hidden-layer-At-each-time-step-output-is-produced-through.ppm)

![Image](https://d2l.ai/_images/rnn.svg)

![Image](https://discuss.pytorch.org/uploads/default/original/2X/e/e7496a33d835f085d800ee17c0ade05895a89551.png)

---

## 1Ô∏è‚É£ What is RNN?

A **Recurrent Neural Network (RNN)** is a neural network designed to handle **sequential data** by **remembering past information**.

üîë RNN has a **memory** ‚Üí called **hidden state**.

---

## 2Ô∏è‚É£ Why RNN?

ANN & CNN assume:

* Inputs are **independent**
* No order matters

But in real life:

* Language
* Time series
* Speech
* Stock prices

üëâ **Order matters**

RNN solves this by passing information **from previous time step to next**.

---

## 3Ô∏è‚É£ Sequential Data Examples

| Data        | Sequence       |
| ----------- | -------------- |
| Text        | word by word   |
| Speech      | audio frames   |
| Time series | day by day     |
| Video       | frame by frame |

---

## 4Ô∏è‚É£ RNN Architecture (Core Idea)

At time step `t`:
[
h_t = f(W_x x_t + W_h h_{t-1} + b)
]
[
y_t = W_y h_t
]

Where:

* `x_t` ‚Üí input at time `t`
* `h_t` ‚Üí hidden state (memory)
* `W_x` ‚Üí input weights
* `W_h` ‚Üí recurrent weights
* `b` ‚Üí bias

---

## 5Ô∏è‚É£ Key Components of RNN

### üîπ Input (`x‚Çú`)

* Current element of sequence

### üîπ Hidden State (`h‚Çú`)

* Memory of past
* Passed to next time step

### üîπ Output (`y‚Çú`)

* Prediction at time `t`

---

## 6Ô∏è‚É£ Weight Sharing (VERY IMPORTANT)

In RNN:

* **Same weights** are used at **all time steps**

This allows:

* Learning long sequences
* Fewer parameters

---

## 7Ô∏è‚É£ Forward Propagation in RNN

For each time step:

1. Multiply input with weights
2. Add previous hidden state
3. Add bias
4. Apply activation

‚û°Ô∏è This repeats for every time step

---

## 8Ô∏è‚É£ Activation Functions in RNN

### Hidden State Activation

* **Tanh** (most common)
* Sometimes ReLU

Why Tanh?

* Output range (-1,1)
* Keeps values stable

---

### Output Activation (depends on task)

| Task                       | Activation |
| -------------------------- | ---------- |
| Binary classification      | Sigmoid    |
| Multi-class classification | Softmax    |
| Regression                 | Linear     |

---

## 9Ô∏è‚É£ Types of RNN Outputs (IMPORTANT)

### 1Ô∏è‚É£ One-to-One

* ANN-like
* Not sequence-based

---

### 2Ô∏è‚É£ One-to-Many

Example:

* Image ‚Üí Caption

---

### 3Ô∏è‚É£ Many-to-One

Example:

* Sentiment analysis

---

### 4Ô∏è‚É£ Many-to-Many

Example:

* Language translation

---

## üîü Loss Functions in RNN

| Task                       | Loss Function            |
| -------------------------- | ------------------------ |
| Binary classification      | Binary Crossentropy      |
| Multi-class classification | Categorical Crossentropy |
| Regression                 | MSE                      |

Loss can be:

* Calculated at **each time step**
* Or only at **final output**

---

## 1Ô∏è‚É£1Ô∏è‚É£ Backpropagation Through Time (BPTT)

RNN uses **BPTT** instead of normal backpropagation.

### Steps:

1. Unroll RNN through time
2. Compute loss
3. Backpropagate errors backward in time
4. Update shared weights

---

## 1Ô∏è‚É£2Ô∏è‚É£ Vanishing Gradient Problem üö®

### What happens?

* Gradients become very small
* Early time steps stop learning

### Why?

* Repeated multiplication of small numbers

üìå **This is the biggest limitation of simple RNN**

---

## 1Ô∏è‚É£3Ô∏è‚É£ Exploding Gradient Problem

* Gradients become very large
* Model becomes unstable

### Solution:

* Gradient clipping

---

## 1Ô∏è‚É£4Ô∏è‚É£ Why Simple RNN Struggles

| Problem              | Reason                 |
| -------------------- | ---------------------- |
| Long-term dependency | Vanishing gradient     |
| Slow training        | Sequential computation |
| Memory loss          | Short-term memory only |

üëâ This is **why LSTM/GRU exist**, but **we stop here** as requested.

---

## 1Ô∏è‚É£5Ô∏è‚É£ RNN Parameters

### Weights:

* Input-to-hidden
* Hidden-to-hidden
* Hidden-to-output

### Bias:

* Shifts activation
* Helps learning

---

## 1Ô∏è‚É£6Ô∏è‚É£ Example: Sentiment Analysis (Many-to-One)

```text
Input: "I love this movie"
‚Üì
Word embeddings ‚Üí RNN
‚Üì
Final hidden state
‚Üì
Sigmoid ‚Üí Positive / Negative
```

---

## 1Ô∏è‚É£7Ô∏è‚É£ Example RNN Architecture (Keras)

```python
SimpleRNN(64, activation='tanh')
Dense(1, activation='sigmoid')
```

---

## 1Ô∏è‚É£8Ô∏è‚É£ When to Use Simple RNN?

‚úÖ Use when:

* Short sequences
* Simple temporal patterns
* Learning basic sequence behavior

‚ùå Avoid when:

* Long sequences
* Long-term dependencies

---

## 1Ô∏è‚É£9Ô∏è‚É£ ANN vs CNN vs RNN (Quick Table)

| Feature        | ANN     | CNN   | RNN      |
| -------------- | ------- | ----- | -------- |
| Data type      | Tabular | Image | Sequence |
| Memory         | ‚ùå       | ‚ùå     | ‚úÖ        |
| Weight sharing | ‚ùå       | ‚úÖ     | ‚úÖ        |
| Temporal info  | ‚ùå       | ‚ùå     | ‚úÖ        |

---

## 2Ô∏è‚É£0Ô∏è‚É£ FINAL MEMORY TABLE üß†

| Term               | Meaning                  |
| ------------------ | ------------------------ |
| Hidden state       | Memory                   |
| Weight sharing     | Same weights across time |
| BPTT               | Learning method          |
| Vanishing gradient | Main RNN problem         |
| Tanh               | Default activation       |

---
