# üìò ARTIFICIAL NEURAL NETWORK (ANN) ‚Äî COMPLETE NOTES

---

## 1Ô∏è‚É£ What is ANN?

An **Artificial Neural Network (ANN)** is a computational model inspired by the **human brain**, made of **neurons**, **weights**, **bias**, and **activation functions** that learn patterns from data.

---

## 2Ô∏è‚É£ Basic Structure of ANN

### Components

* **Input Layer** ‚Üí takes input features
* **Hidden Layer(s)** ‚Üí performs computation
* **Output Layer** ‚Üí produces final result

### Neuron Equation

[
z = \sum (w_i x_i) + b
]
[
a = f(z)
]

Where:

* `x` ‚Üí input
* `w` ‚Üí weight
* `b` ‚Üí bias
* `f` ‚Üí activation function
* `a` ‚Üí neuron output

---

## 3Ô∏è‚É£ Weights and Bias (Very Important)

### üîπ Weight

* Controls **importance** of an input
* Learned during training
* Large weight ‚Üí strong influence

### üîπ Bias

* Shifts the activation function
* Allows model to learn even if inputs are zero
* Improves flexibility

üëâ **Without bias, ANN is weak**

---

## 4Ô∏è‚É£ Forward Propagation (Multiplication happens here)

Steps:

1. Multiply input with weights ‚Üí `x √ó w`
2. Add bias ‚Üí `xw + b`
3. Apply activation function ‚Üí output

This is called **forward propagation**.

---

## 5Ô∏è‚É£ Activation Functions (CORE CONCEPT)

### Why activation functions?

* Introduce **non-linearity**
* Without activation ‚Üí ANN becomes linear (useless)

---

## 6Ô∏è‚É£ Types of Activation Functions

### 1Ô∏è‚É£ Sigmoid

[
\sigma(x) = \frac{1}{1+e^{-x}}
]

üìå Range: (0,1)

‚úÖ Use when:

* Binary classification (output layer)

‚ùå Problems:

* Vanishing gradient
* Slow learning

---

### 2Ô∏è‚É£ Tanh

[
\tanh(x)
]

üìå Range: (-1,1)

‚úÖ Better than sigmoid
‚ùå Still vanishing gradient

---

### 3Ô∏è‚É£ ReLU (MOST IMPORTANT)

[
f(x) = \max(0, x)
]

üìå Range: (0, ‚àû)

‚úÖ Use when:

* Hidden layers (default choice)
* Faster training

‚ùå Problem:

* Dying ReLU (neurons output 0 forever)

---

### 4Ô∏è‚É£ Leaky ReLU

[
f(x) = \max(0.01x, x)
]

‚úÖ Solves dying ReLU problem

---

### 5Ô∏è‚É£ Softmax

[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum e^{x_j}}
]

üìå Range: (0,1), sum = 1

‚úÖ Use when:

* Multi-class classification
* Output layer

---

## 7Ô∏è‚É£ Which Activation Function to Use?

| Layer                        | Activation        |
| ---------------------------- | ----------------- |
| Hidden layer                 | ReLU / Leaky ReLU |
| Binary classification output | Sigmoid           |
| Multi-class output           | Softmax           |
| Regression output            | Linear            |

---

## 8Ô∏è‚É£ Loss Functions (VERY IMPORTANT)

### What is loss?

Loss measures **how wrong** the model is.

---

### 1Ô∏è‚É£ Mean Squared Error (MSE)

[
\frac{1}{n}\sum(y - \hat{y})^2
]

‚úÖ Use when:

* Regression problems

---

### 2Ô∏è‚É£ Binary Cross Entropy

[
-(y\log \hat{y} + (1-y)\log(1-\hat{y}))
]

‚úÖ Use when:

* Binary classification
* Output activation = Sigmoid

---

### 3Ô∏è‚É£ Categorical Cross Entropy

[
-\sum y \log(\hat{y})
]

‚úÖ Use when:

* Multi-class classification
* Output activation = Softmax

---

### üîë Loss Function Selection Rule

| Problem                    | Output Activation | Loss                     |
| -------------------------- | ----------------- | ------------------------ |
| Regression                 | Linear            | MSE                      |
| Binary classification      | Sigmoid           | Binary Crossentropy      |
| Multi-class classification | Softmax           | Categorical Crossentropy |

---

## 9Ô∏è‚É£ Backpropagation (Learning Process)

Steps:

1. Calculate loss
2. Compute gradient of loss wrt weights
3. Update weights:
   [
   w = w - \eta \frac{\partial L}{\partial w}
   ]

Where:

* `Œ∑` = learning rate

---

## üîü Learning Rate

* Controls step size
* Too large ‚Üí unstable
* Too small ‚Üí slow learning

---

## 1Ô∏è‚É£1Ô∏è‚É£ Overfitting & Underfitting

### Overfitting

* Model memorizes data
* Poor test accuracy

### Underfitting

* Model too simple
* Poor training accuracy

---

## 1Ô∏è‚É£2Ô∏è‚É£ Regularization Techniques (Advanced)

### 1Ô∏è‚É£ L1 Regularization

* Makes weights sparse

### 2Ô∏è‚É£ L2 Regularization

* Penalizes large weights
* Most common

---

### 3Ô∏è‚É£ Dropout

* Randomly disables neurons
* Prevents overfitting

---

## 1Ô∏è‚É£3Ô∏è‚É£ Weight Initialization

| Method            | Use            |
| ----------------- | -------------- |
| Random            | Basic          |
| Xavier            | Sigmoid / Tanh |
| He Initialization | ReLU           |

---

## 1Ô∏è‚É£4Ô∏è‚É£ Batch, Epoch, Iteration

* **Batch** ‚Üí subset of data
* **Epoch** ‚Üí full dataset pass
* **Iteration** ‚Üí one batch pass

---

## 1Ô∏è‚É£5Ô∏è‚É£ ANN Classification Judgment

### Binary Classification

* Output neuron = 1
* Sigmoid output > 0.5 ‚Üí class 1

### Multi-Class

* Highest Softmax value ‚Üí predicted class

---

## 1Ô∏è‚É£6Ô∏è‚É£ ANN Architecture Examples

### Binary Classification

```python
Dense(64, activation='relu')
Dense(1, activation='sigmoid')
```

### Multi-class Classification

```python
Dense(64, activation='relu')
Dense(10, activation='softmax')
```

---

## 1Ô∏è‚É£7Ô∏è‚É£ When to Use ANN?

‚úÖ Use ANN when:

* Data is tabular
* Relationships are non-linear
* Feature engineering is hard

‚ùå Avoid ANN when:

* Very small dataset
* Simple linear relationships

---

## üî• FINAL MEMORY TABLE (VERY IMPORTANT)

| Concept         | Purpose            |
| --------------- | ------------------ |
| Weight          | Feature importance |
| Bias            | Shifts output      |
| Activation      | Non-linearity      |
| Loss            | Error measurement  |
| Optimizer       | Weight update      |
| Backpropagation | Learning           |
| Regularization  | Avoid overfitting  |

---

